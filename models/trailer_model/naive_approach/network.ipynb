{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"network.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XYwi6_2F22wO","colab_type":"text"},"source":["# Prediction based on Trailer (naive approach)"]},{"cell_type":"markdown","metadata":{"id":"Jtg5Vtg73BTe","colab_type":"text"},"source":["## Import"]},{"cell_type":"code","metadata":{"id":"JleKJBN04NbQ","colab_type":"code","colab":{}},"source":["import json\n","import os\n","import pickle\n","import urllib\n","\n","import numpy as np\n","import pandas as pd\n","\n","from bs4 import BeautifulSoup\n","\n","try:\n","    from google.colab import drive\n","    drive_dir = '/content/drive'\n","    drive.mount(drive_dir)\n","    os.chdir(\"drive/My Drive/AML/Git_lastClone/neural-network/trailer_model/naive_approach\")\n","    root_dir = '/content/drive/My Drive/AML/'\n","    git_dir = root_dir+'Git/'\n","    COLAB_IN = True\n","except:\n","    COLAB_IN = False\n","\n","if COLAB_IN:\n","    !pip install youtube-dl\n","    !pip install livelossplot\n","    \n","import tensorflow as tf\n","from keras import layers, models, applications\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization, Lambda\n","import cv2\n","from keras import backend as K\n","import numpy as np\n","from tensorflow.python.keras.utils import Sequence\n","import time\n","from sklearn.model_selection import train_test_split\n","from livelossplot.keras import PlotLossesCallback\n","from IPython.display import clear_output\n","clear_output(wait=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2IHWe6UD2pbS"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FVtr2LQR2pDQ"},"source":["### Loader"]},{"cell_type":"code","metadata":{"id":"DL9AjkAt5x0H","colab_type":"code","colab":{}},"source":["!echo \"--force-ipv4\" > /etc/youtube-dl.conf\n","class Entry:\n","  \n","  def __init__(self, **kwargs):\n","    self.movie_id = kwargs['movie_id']\n","    self.name = kwargs['name']\n","    self.revenue_opening = kwargs['revenue_opening']\n","    self.revenue_total = kwargs['revenue_total']\n","    \n","  def __repr__(self):\n","    return f'Name: {self.name}; movie ID: {self.movie_id}'\n","    \n","  def set_metadata(self, metadata_dict):\n","    self.year = metadata_dict['year']\n","    self.genres = metadata_dict['genres']\n","    self.actors = metadata_dict['actors']\n","    self.directors = metadata_dict['directors']\n","    self.creators = metadata_dict['creators']\n","    self.duration = metadata_dict['duration']\n","    \n","  def set_trailers(self, trailer_dict):\n","    self.imdb_trailer = trailer_dict['imdb_trailer']\n","    self.youtube_trailer = trailer_dict['youtube_trailer']\n","    \n"," \n","    \n","\n","class Dataset:\n","  \n","  def __init__(self, dataset_dir):\n","    self.basic_data = pd.read_csv(os.path.join(dataset_dir, 'Dataset.csv'))\n","    self.dataset_dir = dataset_dir\n","    \n","    self.entries = list()\n","    self.movie_indeces = dict()\n","    self._create_entries()\n","    \n","    self.Youtube_urlroot = \"https://www.youtube.com\"\n","    self.Imdb_urlroot = \"https://www.imdb.com\"\n","    \n","  def get_entry(self, movie_id):\n","    return self.entries[self.movie_indeces[movie_id]]\n","  \n","  def get_revenue(self, movie_id):\n","    tmp_df = self.basic_data[self.basic_data['movie_id'] == movie_id]\n","    return tmp_df['revenue_opening'].values[0]\n","  \n","  def load_metadata(self):\n","    with open(os.path.join(self.dataset_dir, 'metadata.data'), 'rb') as fr:\n","      metadata_dict = pickle.load(fr)\n","      \n","    for k, v in metadata_dict['global'].items():\n","      self.__dict__[k] = self._decompress_global_categorical(v)\n","      \n","    for movie_id, movie_metadata in metadata_dict['entries'].items():\n","      entry = self.entries[self.movie_indeces[movie_id]]\n","      entry.set_metadata(movie_metadata)\n","      \n","      # decompress categorical\n","      entry.actors = self._decompress_categorical(entry.actors, len(self.all_actors))\n","      entry.creators = self._decompress_categorical(entry.creators, len(self.all_creators))\n","      entry.directors = self._decompress_categorical(entry.directors, len(self.all_directors))\n","      entry.genres = self._decompress_categorical(entry.genres, len(self.all_genres))\n","    \n","  def load_trailers(self):\n","    with open(os.path.join(self.dataset_dir, 'trailers.data'), 'rb') as fr:\n","      trailers_dict = pickle.load(fr)\n","      \n","    for movie_id, trailers in trailers_dict.items():\n","      entry = self.entries[self.movie_indeces[movie_id]]\n","      entry.set_trailers(trailers)\n","  \n","  def GetTrailer(self, IMDbVideoUrl, YoutubeVideoUrl, trailers_dir, filename): # download trailer\n","        file_output = os.path.join(trailers_dir, filename+\".mp4\")\n","        !youtube-dl -f best --force-ipv4 --merge-output-format mp4 '$YoutubeVideoUrl' -o '$file_output' > /dev/null\n","        \n","            \n","  def _create_entries(self):\n","    for idx, [movie_id, name, rev_open, rev_total] in self.basic_data.iterrows():\n","      entry = Entry(movie_id=movie_id, name=name,\n","                   revenue_opening = rev_open,\n","                   revenue_total = rev_total)\n","      self.entries.append(entry)\n","      self.movie_indeces[movie_id] = idx\n","      \n","  def _compress_global_categorical(self, input_dict):\n","    output_dict = dict()\n","    for k, v in input_dict.items():\n","      output_dict[k] = np.argmax(v)\n","\n","    return output_dict\n","  \n","  def _decompress_global_categorical(self, input_dict):\n","    output_dict = dict()\n","    for k, v in input_dict.items():\n","      indicator = np.zeros(len(input_dict))\n","      indicator[v] = 1\n","      output_dict[k] = indicator\n","      \n","    return output_dict\n","  \n","  def _compress_categorical(self, arr):\n","    dct = dict()\n","    for idx, val in enumerate(arr):\n","      if val > 0:\n","        dct[idx] = val\n","\n","    return dct\n","  \n","  def _decompress_categorical(self, arr, total_len):\n","    indicator = np.zeros(total_len)\n","    for idx, val in arr.items():\n","      indicator[idx] = val\n","    \n","    return indicator"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-uOnXsQDzIh","colab_type":"code","colab":{}},"source":["movies = Dataset(dataset_dir = '../../dataset/')\n","\n","movies.load_trailers()\n","movies.load_metadata()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXCTqz6BW1Lh","colab_type":"text"},"source":["## Naive"]},{"cell_type":"code","metadata":{"id":"mUWHbjsqYWzb","colab_type":"code","colab":{}},"source":["import numpy as np\n","import keras\n","trailers_dir = './'\n","class DataGenerator(keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, movies, labels, batch_size, n_classes, list_IDs, dim=(300, 300), n_channels=3, shuffle=True):\n","        'Initialization'\n","        self.movies = movies\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.list_IDs = list_IDs\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","        self.labels = labels\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","        \n","        \n","\n","        # Generate data\n","        X, y = self.__data_generation(list_IDs_temp)\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, list_IDs_temp):\n","        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n","        # Initialization\n","        X = []\n","        y =[]\n","       \n","        for i, ID in enumerate(list_IDs_temp):\n","           \n","            broken = False\n","            print(\"ID\", ID)\n","            # Store sample\n","            entry = self.movies.entries[ID]\n","            #print(self.movies.entries[ID])\n","            videoFile = os.path.join(trailers_dir, entry.movie_id +'.mp4')\n","            #print('Retrieving movie ', entry.movie_id)\n","            if not os.path.isfile(videoFile):\n","                print('Donwload trailer')\n","                movies.GetTrailer(entry.imdb_trailer, entry.youtube_trailer, trailers_dir, entry.movie_id)\n","                #time.sleep(5)\n","                timer = time.time()\n","                while not os.path.isfile(videoFile):\n","                    #print('Trailer is not yet ready')\n","                    time.sleep(1)\n","                    if(time.time() - timer > 20):\n","                        broken = True\n","                        break\n","            if broken:\n","                continue\n","            print('trailer downloaded')\n","            \n","            #fps\n","            fps=1\n","            #read the video from the file\n","            cap = cv2.VideoCapture(videoFile)\n","            cap.set(cv2.CAP_PROP_FPS, 15)\n","            n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","            video_fps = cap.get(cv2.CAP_PROP_FPS)\n","            ret = True\n","            frame_index = -1\n","            buf = []\n","            fc = 0\n","            print(\"video procesing\")\n","            while ret:\n","                try:\n","                    #next frame\n","                    frame_index = frame_index + 1   \n","                    #read the frame\n","                    ret, frame = cap.read()\n","\n","\n","                    if video_fps > fps and frame_index % int(video_fps / fps) != 0:\n","                        continue\n","\n","                    if ret:\n","                        ret, im = cap.read()\n","                        im = cv2.resize(im, self.dim)\n","                        im = (im / 255.).astype(np.float32)\n","\n","                        buf.append(im)\n","                except:\n","                    print(\"error\")\n","                    continue\n","                \n","            #os.remove(os.path.join(trailers_dir, entry.movie_id+'.mp4'))\n","            X.append(np.array(buf))\n","            y.append(self.labels[ID])\n","            #print(X[i-1].shape)\n","        return np.array(X), np.array(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"urNyuYLhJfYw","colab_type":"code","colab":{}},"source":["class InceptionV3LSTM:\n","    def __init__(self):\n","        pass\n","    \n","    # ****** model ******\n","    def CreateModel(self, resolution, n_classes):\n","        # set learning phase to 0\n","        K.set_learning_phase(0)\n","        self.n_classes = n_classes\n","\n","\n","        video = layers.Input(shape=(None,)+resolution+(3,),name='video_input')\n","        cnn = applications.InceptionV3(\n","            weights='imagenet',\n","            include_top=False,\n","            pooling='avg'\n","        )\n","        cnn.trainable = False\n","        # wrap cnn into Lambda and pass it into TimeDistributed\n","        encoded_frame = layers.TimeDistributed(Lambda(lambda x: cnn(x)))(video)\n","        encoded_vid = layers.LSTM(128)(encoded_frame)\n","        outputs = layers.Dense(n_classes, activation='softmax')(encoded_vid)\n","        self.model = models.Model(inputs=[video],outputs=outputs)\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QPBbhTasHWAV","colab_type":"code","colab":{}},"source":["n_classes = 10\n","resolution = (200, 200)\n","naive_classifier = InceptionV3LSTM()\n","naive_classifier.CreateModel(resolution = resolution, n_classes = n_classes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cjwtbKUG-c7V","colab_type":"code","colab":{}},"source":["def labels(num_classes, revenues):\n","    # non linear labeling\n","    quantiles = np.linspace(1./num_classes, 1, num_classes)\n","    class_limits = [0,]\n","    for q in quantiles:\n","        class_limits.append(np.quantile(revenues, q))\n","\n","    return np.array(class_limits)\n","\n","def set_labels(class_limits, revenues):\n","    return np.sum(class_limits.reshape(class_limits.size, 1) < revenues, 0) - 1\n","\n","def onehot(values):\n","    n_values = np.max(values) + 1\n","    return np.eye(n_values)[values]\n","\n","revenuesRaw = [movies.entries[i].revenue_opening for i in range(len(movies.entries))]\n","revenuesOneHot = onehot(set_labels(labels(n_classes, revenuesRaw), revenuesRaw))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xLuifIzZFroK","colab_type":"code","colab":{}},"source":["naive_classifier.model.compile(optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True),loss='mse', metrics=['acc', 'mse']) #loss='categorical_crossentropy'\n","\n","\n","# Parameters\n","params = {'dim': resolution,\n","          'batch_size': 1,\n","          'n_classes': n_classes,\n","          'n_channels': 3,\n","          'shuffle': True,\n","         }\n","\n","\n","# suffle\n","idx = np.random.permutation(len(movies.entries))[:2000]\n","revenuesRaw = np.array(revenuesRaw)\n","revenuesOneHot = revenuesOneHot[idx]\n","movies.entries = [movies.entries[i] for i in idx] \n","\n","# logger\n","\n","csv_logger = keras.callbacks.CSVLogger('./naive/log.csv', separator=',', append=False)\n","tbCallBack = keras.callbacks.TensorBoard(log_dir='./naive/graph/', histogram_freq=0, write_graph=True, write_images=True)\n","modelCallback = keras.callbacks.ModelCheckpoint('./naive/weights', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n","\n","# Generators\n","training_generator = DataGenerator(movies, revenuesOneHot, list_IDs = list(range(0, 100)), **params)\n","validation_generator = DataGenerator(movies, revenuesOneHot, list_IDs = list(range(0, 100)), **params)\n","# Train model on dataset\n","train_history = naive_classifier.model.fit_generator(generator=training_generator,\n","                    #validation_data=validation_generator,\n","                    shuffle=True,\n","                    max_queue_size=1,\n","                    use_multiprocessing=False, verbose = 1, callbacks=[tbCallBack, csv_logger, modelCallback])"],"execution_count":0,"outputs":[]}]}